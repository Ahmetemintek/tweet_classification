{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster Tweets Classification By Using SparkNLP\n",
    "In this project, I am challenged to build a classification model that predicts which Tweets are about real disasters and which one’s aren’t. <br/>\n",
    "I have access to a dataset of 10,000 tweets that were hand classified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Useful Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting Sparknlp\n",
    "spark= sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkNLP version: 2.6.1\n",
      "Pyspark version: 2.4.4\n"
     ]
    }
   ],
   "source": [
    "print(\"SparkNLP version: {}\".format(sparknlp.version()))\n",
    "print(\"Pyspark version: {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading train and test datasets\n",
    "df_train= spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .csv(\"train.csv\")\n",
    "df_test= spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .csv(\"test.csv\")\n",
    "submission= spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|id |keyword|location|text                                                                                                                                 |target|\n",
      "+---+-------+--------+-------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|1  |null   |null    |Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all                                                                |1     |\n",
      "|4  |null   |null    |Forest fire near La Ronge Sask. Canada                                                                                               |1     |\n",
      "|5  |null   |null    |All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected|1     |\n",
      "|6  |null   |null    |13,000 people receive #wildfires evacuation orders in California                                                                     |1     |\n",
      "|7  |null   |null    |Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school                                              |1     |\n",
      "+---+-------+--------+-------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(5, truncate=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**id:** This column consist ids per each tweets <br/>\n",
    "**keyword:** A keyword from that tweet (although this may be blank) <br/>\n",
    "**location:** The location the tweet was sent from (may also be blank) <br/>\n",
    "**text:** The text of a tweet <br/>\n",
    "**target:** In train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0) <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|target|count|\n",
      "+------+-----+\n",
      "|  null| 1211|\n",
      "|     1| 3081|\n",
      "|     0| 4095|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.groupby(\"target\")\\\n",
    "    .count()\\\n",
    "    .orderBy(col(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data include some null values. I will drop them. Also, firstly I will drop keyword and location columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+\n",
      "| id|                text|target|\n",
      "+---+--------------------+------+\n",
      "| 48|@bbcmtd Wholesale...|     1|\n",
      "| 49|We always try to ...|     0|\n",
      "| 50|#AFRICANBAZE: Bre...|     1|\n",
      "| 52|Crying out for mo...|     0|\n",
      "| 53|On plus side LOOK...|     0|\n",
      "| 54|@PhDSquares #mufc...|     0|\n",
      "| 55|INEC Office in Ab...|     1|\n",
      "| 57|Ablaze for you Lo...|     0|\n",
      "| 59|Check these out: ...|     0|\n",
      "| 62|Had an awesome ti...|     0|\n",
      "| 66|How the West was ...|     1|\n",
      "| 68|Check these out: ...|     0|\n",
      "| 71|First night with ...|     0|\n",
      "| 73|Deputies: Man sho...|     1|\n",
      "| 76|SANTA CRUZ ÛÓ He...|     0|\n",
      "| 77|Police: Arsonist ...|     1|\n",
      "| 78|Noches El-Bestia ...|     0|\n",
      "| 79|#Kurds trampling ...|     1|\n",
      "| 80|TRUCK ABLAZE : R2...|     1|\n",
      "| 81|Set our hearts ab...|     0|\n",
      "+---+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_col= [\"keyword\", \"location\"]\n",
    "df_train= df_train.drop(*drop_col)\n",
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|target|count|\n",
      "+------+-----+\n",
      "|     1| 2062|\n",
      "|     0| 2709|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dropping the values which is null in the target column.\n",
    "df_train= df_train.na.drop(how=\"any\")\n",
    "df_train.groupby(\"target\")\\\n",
    "    .count()\\\n",
    "    .orderBy(col(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there aren't any null values. <br/>\n",
    "Firstly I will apply SparkNLP DocumentAssembler. DocumentAssembler is a entry point to SparkNLP pipeline. <br/>\n",
    "After thar, I will apply Universal Sentence Encoder and then create ClassifierDL. <br/>\n",
    "Finally I will put into the pipeline and fit with the train_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923,7 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "use = UniversalSentenceEncoder.pretrained()\\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "classsifierdl = ClassifierDLApproach()\\\n",
    "    .setInputCols([\"sentence_embeddings\"])\\\n",
    "    .setOutputCol(\"class\")\\\n",
    "    .setLabelColumn(\"target\")\\\n",
    "    .setMaxEpochs(10)\\\n",
    "    .setEnableOutputLogs(True)\\\n",
    "    .setLr(0.004)\\\n",
    "\n",
    "nlpPipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        use,\n",
    "        classsifierdl\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (3834, 3)\n",
      "Test set shape: (937, 3)\n"
     ]
    }
   ],
   "source": [
    "#splitting the data into train set and test set\n",
    "(train_set, test_set)= df_train.randomSplit([0.8, 0.2], seed=100)\n",
    "print(\"Train set shape: {}\".format((train_set.count(), len(train_set.columns))))\n",
    "print(\"Test set shape: {}\".format((test_set.count(), len(test_set.columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting with train_set\n",
    "use_model = nlpPipeline.fit(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we fit pipeline, Spark NLP will write the training logs to \"annotator_logs\" folder in our home directory. <br/>\n",
    "Here is how you can read the logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48\n",
      "-rw-r--r--  1 ahmetemintek  staff   447 26 Eyl 12:55 ClassifierDLApproach_70b0c4c2a95a.log\n",
      "-rw-r--r--  1 ahmetemintek  staff   445 27 Eyl 12:31 ClassifierDLApproach_94dd5e89b5b4.log\n",
      "-rw-r--r--  1 ahmetemintek  staff   797 27 Eyl 12:45 ClassifierDLApproach_a15f773cd2bd.log\n",
      "-rw-r--r--  1 ahmetemintek  staff   889 27 Eyl 12:42 ClassifierDLApproach_d968d7735acf.log\n",
      "-rw-r--r--  1 ahmetemintek  staff   445 27 Eyl 01:46 ClassifierDLApproach_f3c2764d4358.log\n",
      "-rw-r--r--  1 ahmetemintek  staff  1579 27 Eyl 12:47 ClassifierDLApproach_f8faa19c5d43.log\n"
     ]
    }
   ],
   "source": [
    "!cd ~/annotator_logs && ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For check the result of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started - epochs: 10 - learning_rate: 0.004 - batch_size: 64 - training_examples: 4771 - classes: 2\n",
      "Epoch 0/10 - 1,70s - loss: 31.218817 - acc: 0.7941361 - batches: 75\n",
      "Epoch 1/10 - 1,23s - loss: 30.818155 - acc: 0.8378439 - batches: 75\n",
      "Epoch 2/10 - 1,19s - loss: 31.518713 - acc: 0.8504766 - batches: 75\n",
      "Epoch 3/10 - 1,16s - loss: 31.563896 - acc: 0.8565999 - batches: 75\n",
      "Epoch 4/10 - 1,17s - loss: 31.516844 - acc: 0.86272323 - batches: 75\n",
      "Epoch 5/10 - 1,17s - loss: 31.103827 - acc: 0.8684242 - batches: 75\n",
      "Epoch 6/10 - 1,19s - loss: 30.753832 - acc: 0.87264717 - batches: 75\n",
      "Epoch 7/10 - 1,17s - loss: 30.671005 - acc: 0.8762005 - batches: 75\n",
      "Epoch 8/10 - 1,18s - loss: 30.478905 - acc: 0.8785231 - batches: 75\n",
      "Epoch 9/10 - 1,18s - loss: 30.123034 - acc: 0.8795789 - batches: 75\n",
      "Training started - epochs: 10 - learning_rate: 0.004 - batch_size: 64 - training_examples: 3834 - classes: 2\n",
      "Epoch 0/10 - 1,49s - loss: 30.006615 - acc: 0.7987745 - batches: 60\n",
      "Epoch 1/10 - 0,94s - loss: 26.760681 - acc: 0.8372297 - batches: 60\n",
      "Epoch 2/10 - 0,97s - loss: 25.660847 - acc: 0.8451472 - batches: 60\n",
      "Epoch 3/10 - 0,94s - loss: 25.620474 - acc: 0.85600525 - batches: 60\n",
      "Epoch 4/10 - 0,94s - loss: 25.39091 - acc: 0.8594481 - batches: 60\n",
      "Epoch 5/10 - 0,94s - loss: 25.25635 - acc: 0.8633931 - batches: 60\n",
      "Epoch 6/10 - 0,94s - loss: 25.303349 - acc: 0.8641328 - batches: 60\n",
      "Epoch 7/10 - 0,94s - loss: 25.511267 - acc: 0.8614571 - batches: 60\n",
      "Epoch 8/10 - 0,97s - loss: 25.714092 - acc: 0.8650095 - batches: 60\n",
      "Epoch 9/10 - 0,94s - loss: 25.643711 - acc: 0.8708358 - batches: 60\n"
     ]
    }
   ],
   "source": [
    "!cat ~/annotator_logs/ClassifierDLApproach_f8faa19c5d43.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved %87 accuracy score on train_set. <br/>\n",
    "Let's check the model with test_set by using sklearn metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|target|text                                                                                                                                    |result|\n",
      "+------+----------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|1     |Gail and Russell saw lots of hail at their Dalroy home - they have video of twister 1/2 mile from their home #yyc http://t.co/3VfKEdGrsO|[1]   |\n",
      "|0     |Crazy Mom Threw Teen Daughter a NUDE Twister Sex Party According To Her Friend59 more pics http://t.co/t94LNfwf34 http://t.co/roCyyEI2dM|[0]   |\n",
      "|0     |The Sharper Image Viper 24' Hardside Twister (Black) http://t.co/FXk3zsj2PE                                                             |[0]   |\n",
      "|0     |Why Some Traffic Is Freezing Cold And Some Blazing Hot ÛÒ And How To Heat Up Some Of Your Traffic http://t.co/C8b6DdiQIg               |[1]   |\n",
      "|0     |@briannafrost Twister with Bill Paxton and Helen Hunt!                                                                                  |[0]   |\n",
      "+------+----------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds= use_model.transform(test_set)\n",
    "preds.select(\"target\", \"text\", \"class.result\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84       539\n",
      "           1       0.82      0.72      0.77       398\n",
      "\n",
      "    accuracy                           0.81       937\n",
      "   macro avg       0.81      0.80      0.81       937\n",
      "weighted avg       0.81      0.81      0.81       937\n",
      "\n",
      "0.8132337246531484\n"
     ]
    }
   ],
   "source": [
    "df= use_model.transform(test_set).select(\"target\", \"document\", \"class.result\").toPandas()\n",
    "df[\"result\"]= df[\"result\"].apply(lambda x: x[0])\n",
    "print(classification_report(df[\"target\"], df[\"result\"]))\n",
    "print(accuracy_score(df[\"target\"], df[\"result\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved %81 accuracy score on test_set as well. </br>\n",
    "####  Applying the model on the test.csv data for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test= spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+------------------------------------------------------------------------------------------------+\n",
      "|id |keyword|location|text                                                                                            |\n",
      "+---+-------+--------+------------------------------------------------------------------------------------------------+\n",
      "|0  |null   |null    |Just happened a terrible car crash                                                              |\n",
      "|2  |null   |null    |Heard about #earthquake is different cities, stay safe everyone.                                |\n",
      "|3  |null   |null    |there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all|\n",
      "|9  |null   |null    |Apocalypse lighting. #Spokane #wildfires                                                        |\n",
      "|11 |null   |null    |Typhoon Soudelor kills 28 in China and Taiwan                                                   |\n",
      "+---+-------+--------+------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show(5, truncate=False)  #this is the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|id |target|\n",
      "+---+------+\n",
      "|0  |0     |\n",
      "|2  |0     |\n",
      "|3  |0     |\n",
      "|9  |0     |\n",
      "|11 |0     |\n",
      "+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "submission.show(5, truncate=False) #this is the submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------------------------------------+------+\n",
      "|id |text                                                                                            |result|\n",
      "+---+------------------------------------------------------------------------------------------------+------+\n",
      "|0  |Just happened a terrible car crash                                                              |[1]   |\n",
      "|2  |Heard about #earthquake is different cities, stay safe everyone.                                |[1]   |\n",
      "|3  |there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all|[1]   |\n",
      "|9  |Apocalypse lighting. #Spokane #wildfires                                                        |[1]   |\n",
      "|11 |Typhoon Soudelor kills 28 in China and Taiwan                                                   |[1]   |\n",
      "+---+------------------------------------------------------------------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds= use_model.transform(df_test)\n",
    "preds.select(\"id\",\"text\", \"class.result\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = use_model.transform(df_test).select(\"id\", \"class.result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|target|\n",
      "+---+------+\n",
      "|  0|   [1]|\n",
      "|  2|   [1]|\n",
      "|  3|   [1]|\n",
      "|  9|   [1]|\n",
      "| 11|   [1]|\n",
      "+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is the final step\n",
    "final= final.withColumnRenamed(\"result\" ,\"target\")\n",
    "final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-nlp",
   "language": "python",
   "name": "project-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
